{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project: 2018 Match Analysis - First Robotics Compition\n",
    "\n",
    "## Where is your data from?\n",
    "#### [Dataset From Kaggle https://www.kaggle.com/samcfuchs/frc-2018](https://www.kaggle.com/samcfuchs/frc-2018)\n",
    "\n",
    "\n",
    "## Event Details\n",
    "[2018 FIRST Robotics Competition - FIRST POWER UP Game PREVIEW](https://www.youtube.com/watch?v=HZbdwYiCY74)\n",
    "\n",
    "[2018 FRCGame Season Manual](https://firstfrc.blob.core.windows.net/frc2018/Manual/2018FRCGameSeasonManual.pdf)\n",
    "\n",
    "[PNW District Auburn Event 2018](https://www.thebluealliance.com/event/2018waahs)\n",
    "\n",
    "[PNW District Auburn Event Insights 2018](https://www.thebluealliance.com/event/2018waahs#event-insights)\n",
    "\n",
    "[PNW District Auburn Mountainview Event 2018](https://www.thebluealliance.com/event/2018waamv)\n",
    "\n",
    "[PNW District Auburn Mountainview Event Insights 2018](https://www.thebluealliance.com/event/2018waamv#event-insights)\n",
    "\n",
    "[Team 2927: Ï€Rho Techs](https://www.thebluealliance.com/team/2927/2018)\n",
    "\n",
    "\n",
    "## What were you looking to answer and predictions you wanted to make?\n",
    "1. Points Analysis\n",
    "    - How were points distributed?\n",
    "2. Obsticle Analysis\n",
    "    - What obsticles where used most often?\n",
    "    - What obsticle(s) affected win/lose ratio the most?\n",
    "3. Team Analysis\n",
    "    - What alliances (group of 3 teams) won/lost most often?\n",
    "    - What team won/lost most often?\n",
    "    - What team won/lost least often?\n",
    "4. Location Analysis\n",
    "    - Does region or origin affect win/lose ratio?\n",
    "    - Does playing location affect win/lose ratio?\n",
    "5. Other Analysis\n",
    "\n",
    "\n",
    "## A brief summary of your methodology for answering these questions\n",
    "Using Clustering, Classification, and Regression analysis methods we plan to simplify and deduce the order and magnitude in which something affects the outcome of another.\n",
    "\n",
    "Using Fitted and Embedded Methods to do analysis\n",
    "\n",
    "âˆš 1- Apply supervised or un-supervised models to a dataset (or problem) you are interested in. Investigate variety of steps\n",
    "to make the model better including:\n",
    "  - Hyper-parameter tuning by Grid-search\n",
    "  - Check if dataset is balanced or not -> change the threshold\n",
    "  - Data preprocessing (scaling)\n",
    "  - Dimensionality reduction (PCA) -> train the model based on X_reduced_train and Y_reduced_train\n",
    "  - Eliminate unnecessary features -> Feature Engineering\n",
    "  - Try other models and do the above all steps\n",
    "\n",
    "âˆš 2- Read blogs about Feature Engineering and make your model performance better with variety of Feature Engineering options:\n",
    "- https://towardsdatascience.com/feature-selection-correlation-and-p-value-da8921bfb3cf\n",
    "- https://towardsdatascience.com/feature-selection-with-pandas-e3690ad8504b\n",
    "\n",
    "3- Build a Decision Tree (DT) classifier from Scratch (you can use Pandas or any other Python built-in functions) and provide DT visualization. For any categorical dataset, your function should return the optimal tree with the root and all appropriate leafs, max_depth of the tree and the visualized graph. You can follow the steps we explored in class but should work for any dataset for example if we pass Lens dataset. \n",
    "\n",
    "## Graphs and other visualizations that clearly explain your findings (ideally, conclusions)\n",
    "\n",
    "\n",
    "## Metrics you tracked, the values for each, and how you interpret the results \n",
    "\n",
    "|Metrics          | Value               |\n",
    "|-----------------|---------------------|\n",
    "|accuracy         | 0.8888888888888888  |\n",
    "|error            | 0.1111111111111111  |\n",
    "|recall           | 0.8333333333333334  |\n",
    "|precision        | 1.0                 |\n",
    "|specificity      | 1.0                 |\n",
    "|F1               | 0.9090909090909091  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# import statsmodels.api as sm\n",
    "\n",
    "from sklearn import preprocessing, metrics, preprocessing, svm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, RidgeCV, LassoCV, Ridge, Lasso\n",
    "from sklearn.metrics import r2_score, mean_squared_error, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 1., 1., 0., 0., 1., 0., 1., 0., 0.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "data = pd.read_csv('2018_MatchData.csv').set_index(\"Week\")\n",
    "data = Team2927 = data[data['Team'] == 2927].set_index(['Event', 'Match', 'Alliance'])\n",
    "data = data.drop(['City','State','Country','Time','autoQuestRankingPoint','autoRun','autoSwitchAtZero','endgame','faceTheBossRankingPoint','Robot Number', 'adjustPoints', 'tba_gameData'], axis=1)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "for i in data.columns:\n",
    "    data[i] = label_encoder.fit_transform(data[i]).astype('float64')\n",
    "\n",
    "result = label_encoder.fit_transform(data['result']).astype('float64')\n",
    "# result = data['result']\n",
    "data = data.drop(columns=['result', 'Team'])\n",
    "data\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Filter Method:\n",
    "As the name suggest, in this method, you filter and take only the subset of the relevant features. The model is built after selecting the features. The filtering here is done using correlation matrix and it is most commonly done using Pearson correlation.\n",
    "\n",
    "The correlation coefficient has values between -1 to 1\n",
    "\n",
    "    â€” A value closer to 0 implies weaker correlation (exact 0 implying no correlation)\n",
    "    â€” A value closer to 1 implies stronger positive correlation\n",
    "    â€” A value closer to -1 implies stronger negative correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Using Pearson Correlation\n",
    "plt.figure(figsize=(30,20))\n",
    "corr = data.corr()\n",
    "\n",
    "# sns.heatmap(corr)\n",
    "\n",
    "# Can be great to plot only a half matrix\n",
    "mask = np.triu(np.ones_like(corr, dtype=np.bool))\n",
    "cmap = sns.diverging_palette(200, 40, as_cmap=True)\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap)\n",
    "\n",
    "plt.subplots(figsize=(30,20))\n",
    "sns.heatmap(corr, annot=True, cmap=plt.cm.Reds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we compare the correlation between features and remove features that have a correlation higher than 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "columns = np.full((corr.shape[0],), True, dtype=bool)\n",
    "for i in range(corr.shape[0]):\n",
    "    for j in range(i+1, corr.shape[0]):\n",
    "        if corr.iloc[i,j] >= 0.9:\n",
    "            if columns[j]:\n",
    "                columns[j] = False\n",
    "selected_columns = data.columns[columns]\n",
    "data = data[selected_columns]\n",
    "print(selected_columns)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, the dataset has only those columns with correlation less than 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation with output variable\n",
    "cor_target = abs(corr[\"result\"])\n",
    "#Selecting highly correlated features\n",
    "relevant_features = cor_target[cor_target>0.9]\n",
    "relevant_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As we can see, the features \n",
    "#### [rp, teleopOwnershipPoints, teleopPoints, teleopScaleBoostSec, teleopScaleOwnershipSec, totalPoints, and winMargin] \n",
    "### are highly correlated with the output variable result. \n",
    "\n",
    "Hence we will drop all other features apart from these. However **this is not the end of the process**. One of the assumptions of **linear regression** is that the independent variables need to be uncorrelated with each other. If these variables are correlated with each other, then we need to keep only one of them and drop the rest. So let us check the correlation of selected features with each other. This can be done either by visually checking it from the above correlation matrix or from the code snippet below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'teleopPoints', 'totalPoints',\n",
    "simple_corr = data[['winMargin', 'teleopOwnershipPoints',\n",
    "                    'teleopScaleOwnershipSec', 'rp',\n",
    "                    'vaultBoostPlayed', 'teleopSwitchOwnershipSec',\n",
    "                    'teleopScaleBoostSec']].corr()\n",
    "simple_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.heatmap(simple_corr, annot=True, cmap=plt.cm.Reds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = data.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data.values, result.values, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc=SVC() # The default kernel used by SVC is the gaussian kernel\n",
    "svc.fit(x_train, y_train)\n",
    "prediction = svc.predict(x_test)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, prediction)\n",
    "sum = 0\n",
    "for i in range(cm.shape[0]):\n",
    "    sum += cm[i][i]\n",
    "    \n",
    "accuracy = sum/x_test.shape[0]\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate logistic regression model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# fit model\n",
    "# Train the model with X_train and y_train\n",
    "logreg.fit(x_train, y_train)\n",
    "\n",
    "# Pass X_test into predict method -> call the result as y_pred\n",
    "y_pred = logreg.predict(x_test)\n",
    "print(y_pred)\n",
    "\n",
    "confusion = metrics.confusion_matrix(y_test, y_pred)\n",
    "print(confusion)\n",
    "\n",
    "TP = confusion[1, 1]\n",
    "TN = confusion[0, 0]\n",
    "FP = confusion[0, 1]\n",
    "FN = confusion[1, 0]\n",
    "\n",
    "accuracy=(ğ‘‡ğ‘ƒ+ğ‘‡ğ‘)/(ğ‘‡ğ‘ƒ+ğ‘‡ğ‘+ğ¹ğ‘ƒ+ğ¹ğ‘)\n",
    "print(\"ğ‘ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦:\", accuracy)\n",
    "\n",
    "error=(ğ¹ğ‘ƒ+ğ¹ğ‘)/(ğ‘‡ğ‘ƒ+ğ‘‡ğ‘+ğ¹ğ‘ƒ+ğ¹ğ‘)\n",
    "print(\"ğ‘’ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿ =1âˆ’ğ‘ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦:\", error)\n",
    "\n",
    "ğ‘Ÿğ‘’ğ‘ğ‘ğ‘™ğ‘™=ğ‘‡ğ‘ƒ/(ğ‘‡ğ‘ƒ+ğ¹ğ‘)\n",
    "print(\"ğ‘Ÿğ‘’ğ‘ğ‘ğ‘™ğ‘™\", ğ‘Ÿğ‘’ğ‘ğ‘ğ‘™ğ‘™)\n",
    "\n",
    "ğ‘ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›=ğ‘‡ğ‘ƒ/(ğ‘‡ğ‘ƒ+ğ¹ğ‘ƒ)\n",
    "print(\"ğ‘ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›\", ğ‘ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›)\n",
    "\n",
    "ğ‘†ğ‘ğ‘’ğ‘ğ‘–ğ‘“ğ‘–ğ‘ğ‘–ğ‘¡ğ‘¦=ğ‘‡ğ‘/(ğ‘‡ğ‘+ğ¹ğ‘ƒ)\n",
    "print(\"ğ‘†ğ‘ğ‘’ğ‘ğ‘–ğ‘“ğ‘–ğ‘ğ‘–ğ‘¡ğ‘¦\", ğ‘†ğ‘ğ‘’ğ‘ğ‘–ğ‘“ğ‘–ğ‘ğ‘–ğ‘¡ğ‘¦)\n",
    "\n",
    "F1 = 2*(ğ‘ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› * ğ‘Ÿğ‘’ğ‘ğ‘ğ‘™ğ‘™ )/(ğ‘ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› + ğ‘Ÿğ‘’ğ‘ğ‘ğ‘™ğ‘™)\n",
    "print(\"f1\", F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Embedded Method\n",
    "Embedded methods are iterative in a sense that takes care of each iteration of the model training process and carefully extract those features which contribute the most to the training for a particular iteration. Regularization methods are the most commonly used embedded methods which penalize a feature given a coefficient threshold.\n",
    "\n",
    "Here we will do feature selection using Lasso regularization. If the feature is irrelevant, lasso penalizes itâ€™s coefficient and make it 0. Hence the features with coefficient = 0 are removed and the rest are taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LassoCV()\n",
    "reg.fit(x, y)\n",
    "print(\"Best alpha using built-in LassoCV: %f\" % reg.alpha_)\n",
    "print(\"Best score using built-in LassoCV: %f\" %reg.score(x,y))\n",
    "coef = pd.Series(reg.coef_, index = x.columns)\n",
    "# coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")\n",
    "ct = 0\n",
    "cf = 0\n",
    "for i in coef:\n",
    "    if i != 0:\n",
    "        ct += 1\n",
    "    if i == 0:\n",
    "        cf += 1\n",
    "print(\"Lasso picked \" + str(ct) + \" variables and eliminated the other \" +  str(cf) + \" variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_coef = coef.sort_values()\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (8.0, 10.0)\n",
    "imp_coef.plot(kind = \"barh\")\n",
    "plt.title(\"Feature importance using Lasso Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "\n",
    "# read in the tennis data, need the extra parameters since it's a txt file\n",
    "data = pd.read_csv('2018_MatchData.csv').set_index(\"Week\")\n",
    "data = Team2927 = data[data['Team'] == 2927].set_index(['Event', 'Match', 'Alliance'])\n",
    "\n",
    "# encode the data so we can use it with our decision tree,\n",
    "# by converting categories into numbers\n",
    "data_encoded = data.apply(preprocessing.LabelEncoder().fit_transform)\n",
    "# print(data_encoded)\n",
    "\n",
    "# create our decision tree classifier with entropy\n",
    "clf = DecisionTreeClassifier(criterion='entropy', max_depth=3)\n",
    "\n",
    "# one_hot_data = pd.get_dummies(data[['a', 'b', 'c', 'd']], drop_first=True)\n",
    "# print(one_hot_data)\n",
    "\n",
    "# provide our feature array and target array (1-item),\n",
    "# and train the model using a decision tree\n",
    "clf.fit(data_encoded[[\n",
    "    'City', 'State', 'Country', 'Time', 'Team', 'Robot Number',\n",
    "    'adjustPoints', 'autoOwnershipPoints', 'autoPoints',\n",
    "    'autoQuestRankingPoint', 'autoRun', 'autoRunPoints',\n",
    "    'autoScaleOwnershipSec', 'autoSwitchAtZero', 'autoSwitchOwnershipSec',\n",
    "    'endgamePoints', 'endgame', 'faceTheBossRankingPoint', 'foulCount',\n",
    "    'foulPoints', 'rp', 'tba_gameData', 'techFoulCount',\n",
    "    'teleopOwnershipPoints', 'teleopPoints', 'teleopScaleBoostSec',\n",
    "    'teleopScaleForceSec', 'teleopScaleOwnershipSec',\n",
    "    'teleopSwitchBoostSec', 'teleopSwitchForceSec',\n",
    "    'teleopSwitchOwnershipSec', 'totalPoints', 'vaultBoostPlayed',\n",
    "    'vaultBoostTotal', 'vaultForcePlayed', 'vaultForceTotal',\n",
    "    'vaultLevitatePlayed', 'vaultLevitateTotal', 'vaultPoints',\n",
    "    'winMargin'\n",
    "]], data_encoded['result'])\n",
    "\n",
    "# export our decision tree into data that can be plotted\n",
    "dot_data = export_graphviz(clf, out_file=None, feature_names=[\n",
    "    'City', 'State', 'Country', 'Time', 'Team', 'Robot Number',\n",
    "    'adjustPoints', 'autoOwnershipPoints', 'autoPoints',\n",
    "    'autoQuestRankingPoint', 'autoRun', 'autoRunPoints',\n",
    "    'autoScaleOwnershipSec', 'autoSwitchAtZero', 'autoSwitchOwnershipSec',\n",
    "    'endgamePoints', 'endgame', 'faceTheBossRankingPoint', 'foulCount',\n",
    "    'foulPoints', 'rp', 'tba_gameData', 'techFoulCount',\n",
    "    'teleopOwnershipPoints', 'teleopPoints', 'teleopScaleBoostSec',\n",
    "    'teleopScaleForceSec', 'teleopScaleOwnershipSec',\n",
    "    'teleopSwitchBoostSec', 'teleopSwitchForceSec',\n",
    "    'teleopSwitchOwnershipSec', 'totalPoints', 'vaultBoostPlayed',\n",
    "    'vaultBoostTotal', 'vaultForcePlayed', 'vaultForceTotal',\n",
    "    'vaultLevitatePlayed', 'vaultLevitateTotal', 'vaultPoints',\n",
    "    'winMargin'])\n",
    "\n",
    "# Draw graph\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "graph.write_png('obstical_tree.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![obstical_tree.png](obstical_tree.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "## Result\n",
    "1. winMargin\n",
    "2. teleopOwnershipPoints\n",
    "3. teleopScaleOwnershipsec\n",
    "4. rp\n",
    "5. vaultboostplayed\n",
    "6. teleopSwitchOwnershipSec\n",
    "7. teleopScaleBoostSec\n",
    "\n",
    "\n",
    "## Metrics you tracked, the values for each, and how you interpret the results \n",
    "\n",
    "|Metrics          | Value               |\n",
    "|-----------------|---------------------|\n",
    "|accuracy         | 0.8888888888888888  |\n",
    "|error            | 0.1111111111111111  |\n",
    "|recall           | 0.8333333333333334  |\n",
    "|precision        | 1.0                 |\n",
    "|specificity      | 1.0                 |\n",
    "|F1               | 0.9090909090909091  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (20, 25))\n",
    "j = 0\n",
    "for i in data.columns:\n",
    "    plt.subplot(6, 4, j+1)\n",
    "    j += 1\n",
    "    sns.distplot(data[i][y['result']==0], color='g', label = 'Win')\n",
    "    sns.distplot(data[i][y['result']==1], color='r', label = 'Loss')\n",
    "    plt.legend(loc='best')\n",
    "fig.suptitle('')\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.95)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
